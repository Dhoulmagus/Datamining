\documentclass[12pt,paper=a4,twoside]{scrartcl}
\usepackage[T1]{fontenc}      
\usepackage[utf8]{inputenc}	
\usepackage{ae,aecompl} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[ngerman]{babel}   	
\usepackage{graphicx}	
\usepackage{color}	
\usepackage{epstopdf}   
\usepackage{url}	
\usepackage{bbm}	
\usepackage{listings}	
\usepackage[font=small,labelfont=bf]{caption}       
\usepackage[hidelinks]{hyperref}                    
\usepackage{booktabs}	
\usepackage{mathabx}
\usepackage{bibgerm}	
\usepackage{subcaption}
\usepackage{caption}
\usepackage{floatflt,epsfig}
\usepackage[a4paper, left=1cm, right=1cm, top=1cm]{geometry}

\newcommand\tab[1][1cm]{\hspace*{#1}}

\bibliographystyle{plain} 

\makeindex		


\renewcaptionname{ngerman}{\figurename}{Abb.}
\renewcaptionname{ngerman}{\tablename}{Tab.}

\lstset{language=gnuplot}
\definecolor{dkred}{rgb}{0.6,0,0}
\definecolor{dkgreen}{rgb}{0,0.6,0} 
\definecolor{dkpink}{rgb}{0.6,0,0.6}
\definecolor{dkgray}{rgb}{0.5,0.5,0.5}
\lstset{keywordstyle=\color{blue}, stringstyle=\color{dkred}, commentstyle=\color{dkgreen}, identifierstyle=\color{dkpink}}
\lstset{frame=shadowbox, rulesepcolor=\color{dkgray},numbers=left,breaklines=true}

\begin{document}


\section*{2.16}
    \begin{align*}
Q = &\sum_{i=1}^N \sum_{k=1}^K -\gamma_{k,i} \log\Big[ p_{X|Z}(x_i|k) \cdot p_Z(k) \Big]\\
=& \sum_{i=1}^N \sum_{k=1}^K \gamma_{k,i} \Big(\frac{1}{2}\log[2\pi \det(\Sigma_k)] + \frac{1}{2} (x_i - \mu_k)^T \Sigma_k^{-1} (x_i - \mu_k) - \log[p_Z(k)]\Big)
\end{align*}

Assuming that $Q$ is convex, find the optimal values for $\mu_k$ and $\Sigma_k$

**HINT:** You may use the following general matrix/vector gradient equations (refer to the [matrix cook book by Peterson and Pedersen (2012), p.10-11](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) :
\begin{align}
\nabla_x (x - y)^T W (x - y) &= 2 W (x-y) \\
\nabla_W (x - y)^T W (x - y) &= (x-y)(x-y)^T \\
\nabla_{W^{-1}} \log[\det(W)] &= -W & \text{if $W$ is symmetric and positive semi-definite}
\end{align}


Since we have given that this functionis convex, we can simply calculate the first derivative to find it's minima. 
\begin{gather*}
\nabla_{\mu_k} \sum_{i=1}^N \sum_{k=1}^K \gamma_{k,i} \left(\frac{1}{2}\log[2\pi \det(\Sigma_k)] + \frac{1}{2} (x_i - \mu_k)^T \Sigma_k^{-1} (x_i - \mu_k) - \log[p_Z(k)]\right) \stackrel{!}{=} \vec{0}\\
\Leftrightarrow \sum_{i=1}^N \gamma_{k,i} \left( \nabla_{\mu_k} \frac{1}{2} (x_i - \mu_k)^T \Sigma_k^{-1} (x_i - \mu_k) \right) = 0\\
\Leftrightarrow \sum_{i=1}^N \gamma_{k,i} \left( \Sigma^{-1}_k (x_i - \mu_k) \right) = 0\\
\Leftrightarrow \sum_{i=1}^N \gamma_{k,i} \left( \Sigma^{-1}_k x_i - \Sigma^{-1}_k \mu_k \right) = 0\\
\Leftrightarrow \sum_{i=1}^N \gamma_{k,i} \Sigma^{-1}_k x_i = \sum_{i=1}^N  \gamma_{k,i} \Sigma^{-1}_k \mu_k\\
\Leftrightarrow \Sigma_k^{-1} \sum_{i=1}^N \gamma_{k,i} x_i = \Sigma_k^{-1} \mu_k \sum_{i=1}^N \gamma_{k,i}\\
\Leftrightarrow \frac{\sum_{i=1}^N \gamma_{k,i} x_i}{\sum_{i=1}^N \gamma_{k,i}} = \mu_k
\end{gather*}

\begin{gather*}
\nabla_{\Sigma_k^{-1}} \sum_{i=1}^N \sum_{k=1}^K \gamma_{k,i} \left(\frac{1}{2}\log[2\pi \det(\Sigma_k)] + \frac{1}{2} (x_i - \mu_k)^T \Sigma_k^{-1} (x_i - \mu_k) - \log[p_Z(k)]\right) \stackrel{!}{=} \vec{0}\\
\Leftrightarrow\sum_{i=1}^N  \gamma_{k,i} \left(-\frac{1}{2}\Sigma_k + \frac{1}{2} (x_i - \mu_k)^T (x_i - \mu_k)\right) = 0\\
\Leftarrow\sum_{i=1}^N  \gamma_{k,i} \Sigma_k = \sum_{i=1}^N  \gamma_{k,i} (x_i - \mu_k)^{T} (x_i - \mu_k)\\
\Leftrightarrow \Sigma_k \sum_{i=1}^N  \gamma_{k,i} = \sum_{i=1}^N  \gamma_{k,i} (x_i - \mu_k)^{T} (x_i - \mu_k)\\
\Leftrightarrow \Sigma_k = \frac{\sum_{i=1}^N  \gamma_{k,i} (x_i - \mu_k)^{T} (x_i - \mu_k)}{\sum_{i=1}^N  \gamma_{k,i}}
\end{gather*}




\end{document}
